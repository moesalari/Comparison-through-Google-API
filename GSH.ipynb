{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import os \n",
    "import re\n",
    "import pandas as pd\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.discovery import build\n",
    "import time\n",
    "import connection\n",
    "from datetime import date\n",
    "\n",
    "def gsheet_api_check(SCOPES):\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return creds\n",
    "\n",
    "def pull_sheet_data(SCOPES,SPREADSHEET_ID,RANGE_NAME):\n",
    "    creds = gsheet_api_check(SCOPES)\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().get(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=RANGE_NAME).execute()\n",
    "    values = result.get('values', [])\n",
    "    \n",
    "    if not values:\n",
    "        print('No data found.')\n",
    "    else:\n",
    "        rows = sheet.values().get(spreadsheetId=SPREADSHEET_ID,\n",
    "                                  range=RANGE_NAME).execute()\n",
    "        data = rows.get('values')\n",
    "        print(\"COMPLETE: Data copied\")\n",
    "        return data\n",
    "\n",
    "def get_sheet_name(SCOPES,SPREADSHEET_ID):\n",
    "    title = []\n",
    "    creds = gsheet_api_check(SCOPES)\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "    sheet_metadata = service.spreadsheets().get(spreadsheetId=SPREADSHEET_ID).execute()\n",
    "    sheets = sheet_metadata.get('sheets', '')\n",
    "    number_of_sheets = len(sheets)\n",
    "    for i in range(1, number_of_sheets):\n",
    "        title.append(sheets[i].get(\"properties\", {}).get(\"title\"))\n",
    "    return title\n",
    "\n",
    "def clean_sheet_name(sheets_name,exclude_list):\n",
    "    clean_sheets_name = filter(lambda x:x not in exclude_list, sheets_name)\n",
    "    return clean_sheets_name\n",
    "\n",
    "def ft_field_transformation(field_name):\n",
    "    field_name = re.sub(\"(_{2})\", '_', field_name)\n",
    "    # matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', field_name)\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|'         # Small letter following by a capital letter\n",
    "                           # '(?<=[A-Z])(?=[A-Z][a-z])|'       # Capital letter following by 1-Capital letter and then 2- Small letter Note: at this time fivetran does not support this   \n",
    "                          '(?<=[0-9])(?=([a-z])|([A-Z]))|'     # Digit following by a letter either capital or small \n",
    "                          '(?<=([a-z])|([A-Z]))(?=[0-9])|'     # A letter following by digit \n",
    "                          '$)', field_name)\n",
    "    string_in_list = [m.group(0) for m in matches]\n",
    "    return '_'.join(string_in_list).lower()\n",
    "\n",
    "def bi_fsm_join(bi_df,fsm_df):\n",
    "    fsm_df.rename(columns = {\"Field Name\": \"fsm_field_name\", \"Table Name\": \"fsm_table_name\"},inplace = True)\n",
    "    fsm_df['bi_field_name'] = fsm_df['fsm_field_name'].apply(ft_field_transformation) \n",
    "    fsm_df['bi_table_name'] = fsm_df['fsm_table_name'].apply(ft_field_transformation) \n",
    "    bi_df.rename(columns = {'Type':'bi_dataflow_type'},inplace = True)\n",
    "    merge_dfs = pd.merge(bi_df,fsm_df,left_on=['Field Name','Table Name'], right_on = ['bi_field_name','bi_table_name'],how = 'left')\n",
    "    return merge_dfs \n",
    "\n",
    "def load_dilloit_data_dictionary():\n",
    "    attempt = 1\n",
    "    while attempt < 10:\n",
    "        try:\n",
    "            print('Create deloitte_data_dictionary dataframe:')\n",
    "            print('')\n",
    "            scopes = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "            spreadsheet_id = '1649vVV3WWpdwteB1OdvxzdL8_g8Qnswj84UTT2UiJss'\n",
    "            exclude_list = ['Pricebook_Entry_Priority__c','Sheet89','GuideLines','Object Summary','Object Dependency','DM Loading order','OpportunityLineItem','Franchise_QB_Data__c']\n",
    "            column_names = ['Table Name','Field Name','FSL Destination Object(s)','FSL Destination Field','FSL New?','Ignore?','Migrated?']\n",
    "            sheets_name = get_sheet_name(scopes,spreadsheet_id)\n",
    "            clean_sheets_name = clean_sheet_name(sheets_name,exclude_list)\n",
    "            deloitte_data_dictionary = pd.DataFrame(columns = column_names)\n",
    "            sheet = ''\n",
    "            for sheet in clean_sheets_name:\n",
    "                print(sheet)\n",
    "                data = pull_sheet_data(scopes,spreadsheet_id,sheet)\n",
    "                for i in range(1,len(data)):\n",
    "                    if len(data[i]) != len(data[0]):\n",
    "                        dif = len(data[0]) - len(data[i])\n",
    "                        for x in range(1,dif+1):\n",
    "                            data[i].append('')\n",
    "                df_data = pd.DataFrame(data[1:], columns=data[0])\n",
    "                if sheet.find('(') != -1:\n",
    "                    sheet = sheet[0:sheet.find('(')]\n",
    "\n",
    "                df_data['Table Name'] = sheet\n",
    "                deloitte_data_dictionary = deloitte_data_dictionary.append(df_data[column_names])\n",
    "            attempt = 100\n",
    "            print('deloitte data dictionary downloaded successfully')\n",
    "            error_flag_1 = False\n",
    "        except:\n",
    "            attempt = attempt + 1  \n",
    "            print('Warning: attemp number: ', attempt)\n",
    "            print('Wait for 30 seconds')\n",
    "            print('')\n",
    "            time.sleep(30)\n",
    "    if attempt == 10:\n",
    "        print('cannot create deloitte_data_dictionary')\n",
    "        error_flag_1 = True\n",
    "    return error_flag_1, deloitte_data_dictionary\n",
    "\n",
    "def load_bi_data_dictionary():\n",
    "    attempt  =  1\n",
    "    while attempt < 10:\n",
    "        print('Create bi_data_dictionary dataframe:')\n",
    "        try:\n",
    "            print('')\n",
    "            print('attempt number ',attempt )\n",
    "            SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "            SPREADSHEET_ID = '1MtcIBSQuiVIPTbYTGhRm1ZHQFwFux71a5yHe5NDXDtU'\n",
    "            sheet = 'Data Analysis'\n",
    "            data = pull_sheet_data(SCOPES,SPREADSHEET_ID,sheet)\n",
    "            bi_data_dictionary = pd.DataFrame(data[1:], columns=data[0])\n",
    "            attempt = 100\n",
    "            print('BI data dictionary downloaded successfully')\n",
    "            error_flag_2 = False\n",
    "        except:\n",
    "            print('Warning! attempt number: ',attempt, ' was not successful.')\n",
    "            print('Next attemp will be run in 15 seconds.')\n",
    "            attempt = attempt + 1\n",
    "            time.sleep(15)\n",
    "    if attempt == 10:\n",
    "        print('cannot create bi_data_dictionary')\n",
    "        error_flag_2 = True\n",
    "    return error_flag_2,bi_data_dictionary\n",
    "\n",
    "def main_data_dictionary():\n",
    "    error_flag_1,deloitte_data_dictionary = load_dilloit_data_dictionary()\n",
    "    error_flag_2,bi_data_dictionary = load_bi_data_dictionary()\n",
    "    if (error_flag_1 == False and error_flag_2 == False):\n",
    "        joined = bi_fsm_join(bi_data_dictionary,deloitte_data_dictionary)\n",
    "        joined['joined_date'] = date.today().strftime(\"%Y-%m-%d\")\n",
    "        connection.df_to_s3(joined, 'bi_fsm_data_dictionary/'+ date.today().strftime(\"%Y-%m-%d\")+ '_fsm_bi_data_dictionary')\n",
    "        connection.s3_to_redshift('bi_fsm_data_dictionary/'+ date.today().strftime(\"%Y-%m-%d\")+ '_fsm_bi_data_dictionary','engineering_sandbox.bi_dilloit_data_dictionary')\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "main_data_dictionary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
